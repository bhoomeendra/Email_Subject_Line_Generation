{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f226cc6-32c4-4961-a770-b5875312463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from typing import List,Dict,Literal\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a9081ef-7a96-4368-b6f3-3c8a61366cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigDataSet(BaseModel):\n",
    "        split: Literal['train','dev','test']\n",
    "        model_name: Literal['gpt2-large','gpt2-medium','gpt2','gpt2-xl'] = 'gpt2'\n",
    "        trun_limit: int = 500    \n",
    "        BASEPATH : Path = Path(\"../data/\")\n",
    "        debug: bool = True\n",
    "\n",
    "class EnronEmailDataset(Dataset):\n",
    "    # Read About MRO(Method Resolution Order)    \n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ConfigDataSet\n",
    "    ):\n",
    "        # As Config is at as just data we can us it with pydatic\n",
    "        self.config = config\n",
    "        self.tokenizer  = GPT2Tokenizer.from_pretrained(self.config.model_name)\n",
    "        self.file_paths: List[str] = [ self.config.BASEPATH/self.config.split/name \n",
    "                                      for name in \n",
    "                                      os.listdir(self.config.BASEPATH/self.config.split)]\n",
    "        self.emails: List[str] = [ open(self.file_paths[idx],'r').read().strip()\\\n",
    "                                         for idx in tqdm(range(len(self.file_paths)),desc=\"Loading Email\") ]\n",
    "        if self.config.debug:\n",
    "            print(f\"First Data point of {self.config.split} tokenized as :\\n\",self[0])\n",
    "        \n",
    "    def clean_text(\n",
    "        self,\n",
    "        text:str\n",
    "        ):\n",
    "        # Updated so that it comes from config\n",
    "        # ipdb.set_trace()\n",
    "        text = re.sub(' +',' ',text)\n",
    "        text = re.sub('\\n+','\\n',text)\n",
    "        text = re.sub('[^A-Za-z0-9\\n\\s\\\\/.-]+','',text)\n",
    "        return text\n",
    "        \n",
    "        \n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx:int\n",
    "    ):\n",
    "        \n",
    "        \"\"\" \n",
    "        returns the input_ids and attention_maks also tuncates if\n",
    "        the email is longer that what is specified in config\n",
    "        \"\"\"\n",
    "        \n",
    "        # with open(self.file_paths[idx],'r') as f:\n",
    "        #     email_with_subject = f.read().strip()\n",
    "        \n",
    "        email_with_subject = self.emails[idx]\n",
    "\n",
    "        email,subject = email_with_subject.split(\"@subject\\n\")\n",
    "        \n",
    "        email = self.clean_text(email)\n",
    "\n",
    "        email = ''.join(email.split()[:self.config.trun_limit])\n",
    "\n",
    "        return self.tokenizer(email +\"@subject\\n\"+ subject)\n",
    "\n",
    "\n",
    "        # tok_email = self.tokenizer(email,truncation=True,max_length=self.config.trun_limit)\n",
    "        # tok_subject = self.tokenizer( \"@subject\\n\"+ subject + \" <|endoftext|>\",\n",
    "        #                              truncation=True,max_length=self.config.trun_limit)\n",
    "        \n",
    "        # Token from which CLM will start Finetuning\n",
    "        # st_gen_token = len(tok_email['input_ids'])\n",
    "        \n",
    "        # tok_email['input_ids'].extend(tok_subject['input_ids'])\n",
    "        # tok_email['attention_mask'].extend([0]*len(tok_subject['attention_mask']))\n",
    "        \n",
    "        \n",
    "         # return ({'input_ids':torch.tensor(tok_email['input_ids']),\n",
    "         #         \"attention_mask\":torch.tensor(tok_email['attention_mask'])},st_gen_token)\n",
    "\n",
    "        \n",
    "    def __len__(\n",
    "        self\n",
    "    ):\n",
    "        return len(self.file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2639ff1e-001f-4215-af2c-bddac3567228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Email: 100%|███████████████████████████| 14436/14436 [00:35<00:00, 410.99it/s]\n"
     ]
    }
   ],
   "source": [
    "dataconfig = ConfigDataSet( split='train',\n",
    "                            trun_limit=500)\n",
    "dataset = EnronEmailDataset(dataconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e7407b-1551-4829-848e-fe8656b7b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Email: 100%|█████████████████████████████| 1960/1960 [00:07<00:00, 253.72it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataconfig = ConfigDataSet( split='dev',\n",
    "                            trun_limit=500)\n",
    "val_dataset = EnronEmailDataset(val_dataconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82d78826-645b-49bd-acd2-618d5ffc6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=dataset.tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9013721-70cf-45e7-b3db-41ff47cdc2f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'splti'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mval_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 55\u001b[0m, in \u001b[0;36mEnronEmailDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     51\u001b[0m email,subject \u001b[38;5;241m=\u001b[39m email_with_subject\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@subject\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m email \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_text(email)\n\u001b[0;32m---> 55\u001b[0m email \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43memail\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplti\u001b[49m()[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrun_limit])\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(email \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@subject\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m subject)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'splti'"
     ]
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f729e-558a-46c0-b137-6a42f93a6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"data/GPT2_FT_Model\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_path,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4, # try with 2\n",
    "    per_device_eval_batch_size=4,  #  try with 2\n",
    "    num_train_epochs=10,\n",
    "    save_steps=1_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
